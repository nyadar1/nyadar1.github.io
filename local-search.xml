<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>MPC模型推导</title>
    <link href="/2024/10/02/MPC%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC/"/>
    <url>/2024/10/02/MPC%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC/</url>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>本文基于<ahref="https://www.doc88.com/p-54287015838598.html">人机共驾型智能汽车的共享控制方法研究</a>进行学习分析，虽然文章中存在一些错误(Ac不可逆却使用其逆进行离散化等)，但是对模型的整体的推导并无大碍。为了和学长学姐的已有框架一致，对文章中的模型做了一些调整与简化(调整了状态量的选择，删去曲率约束)。</p><h1 id="模型推导与建立">模型推导与建立</h1><p>文章使用道路坐标下的车辆线性二自由度动力学模型，具体如下图： <imgsrc="model.png" alt="二自由度车辆动力学模型" />a是车辆质心距前轴距离、b是车辆质心距后轴距离。在车辆不发生明显质心转移的情况下，一般认为这两个量为常数。V是车辆的纵向速度、v是车辆的横向速度、是车辆的横摆角速度，这三个量构成描述车辆运动的速度量。<span class="math inline">\(\alpha_f\)</span>和<spanclass="math inline">\(\alpha_r\)</span>分别是前后轮侧偏角，表示前后轮胎面与车轮运动方向的夹角(注：在车辆转弯时，车轮朝向一般不是车轮的实际运动方向，车轮会因此出现磨损)。<spanclass="math inline">\(F_{yf}\)</span>与<spanclass="math inline">\(F_{yr}\)</span>分别是前后轮所受路面侧偏力(磨损时地面对轮胎的力)，当车轮垂向载荷及路面附着系数固定的情况下与侧偏角存在一定函数关系，该函数关系称为轮胎的侧偏特性。<spanclass="math inline">\(\delta\)</span>是前轮转向角(前轮朝向与车辆坐标系x轴的夹角)，一般与作为模型输入的方向盘转角u成固定比例<spanclass="math inline">\(\delta = u/i_s\)</span>，其中<spanclass="math inline">\(i_s\)</span>为转向比。</p><p>定义车辆的状态量为<span class="math inline">\(x=[v, \omega, y,psi]\)</span>，v时车辆质心处的纵向速度，<spanclass="math inline">\(\omega\)</span>是质心处的角速度，y为世界坐标系下y方向的距离，psi是车辆坐标系x轴与世界坐标系x轴的夹角。</p><p>根据平面刚体运动理论，车辆质心的横向加速度<spanclass="math inline">\(a_y\)</span>可以表示为： <spanclass="math display">\[a_y = \dot{v}+V\omega\]</span>可以看到是横向速度的变化量加上向心加速度。对前轮运动进行分析，其横向速度分量为<spanclass="math inline">\(v+a\omega\)</span>、纵向速度分量为V，可以看作质心为转轴，车身为连杆的旋转运动。根据前轮侧偏角的定义有：<span class="math display">\[\frac{v+a\omega}{V}=\tan{\delta-\alpha_f}\]</span> 对上式进行变换，有： <span class="math display">\[\alpha_f = \delta- \arctan{\frac{v+a\omega}{V}}\]</span> 以同样的方法对后轮运动进行分析，其横向速度分量为<spanclass="math inline">\(v-b\omega\)</span>、纵向速度分量为V，注意到这里我们的对<spanclass="math inline">\(\alpha_r\)</span>与<spanclass="math inline">\(\alpha_f\)</span>的定义，是从车轮处实际运动方向指向车轮朝向，逆时针为正，因此画一下图不难发现这里是<spanclass="math inline">\(v-b\omega\)</span>而非加号。然后由于后轮不会转向，其朝向一直为车辆坐标系的x轴，所以后轮的<spanclass="math inline">\(\delta\)</span>恒为0。所以根据后轮侧偏角的定义有：<span class="math display">\[\alpha_r = -\arctan{\frac{v-b\omega}{V}}\]</span>对二自由度车辆模型的横向运动和横摆运动分别列写动力学微分方程(F=ma，合外力矩=转动惯量*角加速度)，有：<span class="math display">\[2F_{yf}\cos{\delta}+2F_{yr}=ma_y=m(\dot{v}+V\omega)\]</span> <span class="math display">\[2aF_{yf}\cos{\delta}-2bF_{yr}=I_z\dot{w}\]</span> 式中，<span class="math inline">\(m\)</span>为车身质量、<spanclass="math inline">\(I_z\)</span>为车身转动惯量。由于自行车模型将左右车轮进行了集中等效处理，因此车辆前后两端所受侧向力为相应轮胎侧偏力的两倍。这两个等式对着示意图看还是很清楚的，均是对质心列写的方程。</p><h2 id="小角度假设">小角度假设</h2><h4 id="车辆前轮转向角delta在小范围内delta5变化">车辆前轮转向角<spanclass="math inline">\(\delta\)</span>在小范围内（<spanclass="math inline">\(\delta\)</span>&lt;5°）变化</h4><p>一般在平缓道路进行车道保持时，前轮转角的小角度假设都是成立的。在该假设下，有:<span class="math display">\[\begin{cases}\cos{\delta}\approx 1 \\\\\alpha_f \approx \delta- {\frac{v+a\omega}{V}} \\\\\alpha_r \approx -{\frac{v-b\omega}{V}}\notag% \notag 取消编号\end{cases}\]</span> 另外，前轮转角的小角度假设会使得轮胎侧偏角<spanclass="math inline">\(\alpha_f\)</span>和<spanclass="math inline">\(\alpha_r\)</span>同样在小角度范围内变化。此时，前后轮均工作于线性区间，轮侧偏力与侧偏角之间存在近似线性关系：<spanclass="math inline">\(F_{yf} =C_f\alpha_f\)</span>、<spanclass="math inline">\(F_{yr} = C_r\alpha_r\)</span>，其中<spanclass="math inline">\(C_f\)</span>和<spanclass="math inline">\(C_r\)</span>分别表示前后轮侧偏刚度。将这些式子代入前面的动力学微分方程，并代入u=is，可得：<span class="math display">\[\begin{cases}\dot{v}=-\frac{2(C_f+C_r)}{MV}v-[\frac{2(aC_f-bC_r)}{mV}+V]\omega+\frac{2C_f}{i_sm}u\\\\\dot{\omega}=-\frac{2(aC_f-bC_r)}{I_zV}v-\frac{2(a^2C_f+2b^2C_r)}{I_zV}\omega+\frac{2aC_f}{i_sI_z}u\notag% \notag 取消编号\end{cases}\]</span> 考虑到车辆状态<span class="math inline">\(x=[v, \omega, y,psi]\)</span>，可将其表达为如下线性定常形式： <spanclass="math display">\[\dot{x}=A_cx+B_cu\]</span> 其中 <span class="math display">\[A_c=\begin{bmatrix}\frac{-2(C_f+C_r)}{MV}&amp;\frac{-2(aC_f-bC_r)}{mV} -V&amp;0&amp;0\\\frac{-2(aC_f-bC_r)}{I_zV}&amp;\frac{-2(a^2C_f+2b^2C_r)}{I_zV}&amp;0&amp;0\\1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\end{bmatrix}\]</span> <span class="math display">\[B_c=\begin{bmatrix}\frac{2C_f}{i_sm}\\\frac{2aC_f}{i_sI_z}\\0\\0\end{bmatrix}\]</span> 采用欧拉法对其进行离散化，具体的<spanclass="math inline">\(\dot{x}=\frac{x_{k+1}-x_{k}}{T_s}\)</span>，<spanclass="math inline">\(T_s\)</span>为时间步长，可以得到： <spanclass="math display">\[\begin{cases}A=A_cT_s+I(4)\\B=B_cT_s\end{cases}\]</span> 其中<spanclass="math inline">\(I(4)\)</span>表示4x4的单位矩阵。</p><h1id="无约束模型预测轨迹跟踪算法的构造与求解">无约束模型预测轨迹跟踪算法的构造与求解</h1><p>定义 <span class="math display">\[z=\begin{bmatrix}y\\psi\end{bmatrix}\]</span> 且<spanclass="math inline">\(z_{k+i|k}\)</span>表示在第k步预测第k+i步的z值，这里就是MPC预测框中的预测变量。由z的定义我们很容易得到：<span class="math display">\[z_{k+i|k} = Cx_{k+i|k}\]</span> <span class="math display">\[C=\begin{bmatrix}0&amp;0&amp;1&amp;0\\0&amp;0&amp;0&amp;1\end{bmatrix}\]</span> 用Q与R矩阵作为惩罚系数，可以得到约束项与优化项，其中<spanclass="math inline">\(z_{r,k+i|k}\)</span>为对应时刻的参考状态项 <spanclass="math display">\[\min_{U_k}(\sum_{i=1}^{N}\parallel z_{k+i|k}-z_{r,k+i|k}\parallel^2_Q+\sum_{i=1}^{N-1}\parallel u_{k+i|k} \parallel^2_R)\]</span> 使得 <span class="math display">\[\begin{cases}x_{k+i+1|k}=Ax_{k+i|k}+Bu_{k+i|k},                i=0,1,\dots,N-1\\z_{k+i|k} = Cx_{k+i|k},                    i=1,2,\dots,N\\x_{k|k} = x_k\end{cases}\]</span>下面文章推导了轨迹跟踪算法的解析解形式。这一节仿照了Maciejowki书中所使用的无约束模型预测控制推导过程求解问题的解析控制律。首先，将预测方程由第k+1|k步迭代至第k+N|k步：<span class="math display">\[\begin{bmatrix}z_{k+1|k}\\z_{k+2|k}\\\vdots \\z_{k+N|k}\end{bmatrix}=\begin{bmatrix}CA\\CA^2\\\vdots \\CA^N\end{bmatrix}x_k+\begin{bmatrix}CB&amp; 0 &amp;\cdots &amp;0\\CAB&amp; CB &amp;\cdots&amp;0\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\CA^{N-1}B&amp;CA^{N-2}B&amp;\cdots&amp;CB\end{bmatrix}\begin{bmatrix}u_{k+1|k}\\u_{k+2|k}\\\vdots \\u_{k+N|k}\end{bmatrix}\]</span> 为方便推导，引入如下所示的矩阵和向量符号：</p><p><span class="math display">\[Z_k=\begin{bmatrix}z_{k+1|k}\\z_{k+2|k}\\\vdots \\z_{k+N|k}\end{bmatrix},Z_{r,k}=\begin{bmatrix}z_{r,k+1|k}\\z_{r,k+2|k}\\\vdots \\z_{r,k+N|k}\end{bmatrix}\]</span> <span class="math display">\[\Phi = \begin{bmatrix}CA\\CA^2\\\vdots \\CA^N\end{bmatrix},\Theta = \begin{bmatrix}CB&amp; 0 &amp;\cdots &amp;0\\CAB&amp; CB &amp;\cdots&amp;0\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\CA^{N-1}B&amp;CA^{N-2}B&amp;\cdots&amp;CB\end{bmatrix},Q(R) = \begin{bmatrix}Q&amp;&amp;&amp;\\&amp;Q&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;Q\end{bmatrix}\]</span> 综上，问题可以简化为： <span class="math display">\[Z_k=\Phi x_k+\Theta U_k\]</span> 目标为： <span class="math display">\[\min_{U_k}(\parallel Z_{k}-Z_{r,k} \parallel^2_Q+\parallel U_{k}\parallel^2_R)\]</span> 其解与如下最小二乘问题的解等价： <span class="math display">\[\min_{U_k}\parallel\begin{bmatrix}\sqrt{Q}\Theta\\\sqrt{R}\end{bmatrix}U_k-\begin{bmatrix}\sqrt{Q}\\\sqrt{0}\end{bmatrix}(Z_{r,k}-\Phi x_k)\parallel^2\]</span> 其最小二乘问题的解可表示为： <span class="math display">\[U_k=K(Z_{r.k}-\Phi x_k)\]</span> <span class="math inline">\(K=\begin{bmatrix}\sqrt{Q}\Theta\\\sqrt{R}\end{bmatrix}^{\top}\begin{bmatrix}\sqrt{Q}\\0\end{bmatrix}\)</span>其中表示伪逆，具体的：<spanclass="math inline">\(A^{\top}=((A^TA)^-1A)\)</span>考虑到模型预测控制的原理是取最优输入序列的第一个值作为当前控制量，无约束模型预测控制轨迹跟踪问题的控制律可解析表示为：<span class="math display">\[u_k=\begin{bmatrix}1&amp;0&amp;\cdots&amp;0\end{bmatrix}_NK(Z_{r.k}-\Phi x_k)\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Autonomous driving</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MPC</tag>
      
      <tag>控制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vision Transformer学习笔记</title>
    <link href="/2024/10/01/Transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/10/01/Transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="vision-transformer">Vision Transformer</h1><h2 id="目的与来源">1 目的与来源</h2><p>本科学习期间，参加了一些机器人比赛，主要负责的视觉部分。在比赛结束后，我继续沿着视觉方向，学习了VisionTransformer(ViT)，只看论文总有些隔靴搔痒，不得精髓。于是就有了这篇学习笔记。代码来源于<ahref="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py">vit-pytorch</a>。下面将先分析ViT的总体思路，再深入代码进行学习。<ahref="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf">Transformer论文</a>,<ahref="https://arxiv.org/pdf/2010.11929">ViT论文</a></p><h2 id="网络设计思路">2 网络设计思路</h2><p>ViT的思路来源于Transformer架构，Transformer由2017年Google团队提出，该文原本为自然语言处理领域的成果，后来在深度学习的众多领域都表现出了卓越的能力，如今的火热的大语言模型(如GPT等)基本均以其为基础。</p><p>Transformer原本的结构分为encoder(下图左侧)与decoder(下图右侧)两个部分，而ViT将其迁移至视觉领域，仅保留了encoder部分，删除了decoder，结构其实是简单了很多。可以看到encoder主要由InputEmbedding(词嵌入)，Positional Encoding(位置嵌入)，Multi-HeadAttention(多头注意力模型)，Add &amp; Norm(残差相加与归一化)，FeedForward(前馈层)构成。decoder部分相较于encoder仅仅是多了Masker Multi-HeadAttention(带掩膜的多头注意力模块)和末尾的Linear与Softmax。这些模块在后续的代码分析中都会详细讲解。<img src="Transformer_model.png" alt="Transformer结构图" /></p><p>原本的Transformer处理的文本是序列化的信息(文字存在前后顺序且不可随意变动)，而且将文字嵌入为词向量(将文本编码为网络可处理的向量，一般为一维向量)。如果将一张224x224大小的图像存储在一个一维向量中，这个向量的维度会过于庞大，网络难以处理，所以作者首先将一张图像切成很多的patch，每个patch很小，一般为16x16。例如一张224x224的图像就可以分割为196个16x16的patch。将每个patch拉伸为一个向量，网络是完全可以处理的。在上述例子中，我们就可以得到196个序列向量，再为每个向量添加位置嵌入(告诉网络每个patch的相对位置关系)，送入到ViT中即可。由于ViT被提出时是用于图像分类任务，所以作者在Transfomerencoder的末尾处添加了线性层用于最后的分类任务。 ## 3 代码分析</p><p>代码由4个类和1个函数构成，4个类层层嵌套，实现全过程：<code>class ViT, class Transformer, class Attention, class FeedForward</code>.一个函数为<code>pair(t)</code>，用于判断参数t是否为元组，是，则不变；否则返回<code>(t, t)</code>。下面从最外层类开始逐渐深入分析。</p><h4 id="vit类">ViT类</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,*, image_size, patch_size,num_classes, dim,depth, heads, mlp_dim, pool= <span class="hljs-string">&#x27;cls&#x27;</span>, channels= <span class="hljs-number">3</span> ,dim_head = <span class="hljs-number">64</span> , dropout= <span class="hljs-number">0.</span>, emb_dropout =<span class="hljs-number">0.</span></span>):<br></code></pre></td></tr></table></figure><p>首先在init函数中进行相关的参数初始化。image_size是输入图像大小，patch_size是每个patch的大小，num_classes为分类的种类数，dim参数是一个将patch的维度进行线性映射后的维度。depth是多少个注意力块与前馈网络，heads为被切成大块的q,k,v再被分为多少个小块(头)，mlp_dim是前馈网络中的中间隐藏层维度，pool为程序最后的池化方式（只接受cls与mean两种方式），channels输入图像的通道数，dim_head每个头的维度（与heads配套使用），dropout是transformer块中的参数（同时用在了Attention和FeedForward里），emb_dropout，embedding结束后dropout的参数，dropout主要用于减轻网络过拟合问题。上述参数没有理解也没有关系，后续还会多次出现。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">num_patches=(image_height// patch_height)*(image_width//,patch_width)<br>patch_dim= channels*patch_height* patch_width<br></code></pre></td></tr></table></figure>这里根据参数计算出一共num_patches个patch，每个patch的维度为patch_dim。下面正式进入前向传播。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = <span class="hljs-variable language_">self</span>.to_patch_embedding(img)<br></code></pre></td></tr></table></figure><p>首先将输入图像送入to_patch_embedding，to_patch_embedding定义如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.to_patch_embedding= nn.Sequential(<br>  Rearrange(<span class="hljs-string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_height, p2=patch_width),<br>  nn.LayerNorm(patch_dim),<br>  nn.Linear(patch_dim, dim),<br>  nn.LayerNorm(dim),<br>  )<br></code></pre></td></tr></table></figure> 不难看到，这里实际是将形如[batch_size, channels, height,width]的4维输入张量图像reshape为形如[batch_size, h*w,patch_height*patch_width*channels]的3维张量，注：patch_dim =patch_height * patch_width *channels。再在最后一个维度进行LayerNorm层归一化，映射到dim维，再LayerNorm，成为形如[batch_size,h*w,dim]的张量。这里也不难看出，h与w分别为原图像每一列与每一行的patch个数，所以num_patches= h * w。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">b, n, _ = x.shape<br>cls_tokens= repeat(<span class="hljs-variable language_">self</span>.cls_token,<span class="hljs-string">&#x27;1 1 d -&gt; b 1 d&#x27;</span>, b=b)<br>x= torch.cat((cls_tokens, x), dim= <span class="hljs-number">1</span> )<br>x+= <span class="hljs-variable language_">self</span>.pos_embedding[:, :(n+ <span class="hljs-number">1</span> )]<br>x= <span class="hljs-variable language_">self</span>.dropout(x)<br></code></pre></td></tr></table></figure>x.shape获取大小不再赘述，随后对self.cls_token进行了repeat，初始化中定义<code>self.cls_token = nn.Parameter(torch.randn(1,1, dim))</code>，说明self.cls_token就是一个可训练的随机张量。repeat函数将这个张量重复b次，形成了[batch_size,1, dim]的张量。紧接着，使用cat函数在第1个维度（num_patches）进行了拼接，形成了[batch_size, num_patches+1,dim]的张量。cls_token原本是代表最终类别的标签，但在实践中发现cls_token可以被每个类的概率直接替代。所以目前我认为，这个参数可有可无.....</p><p>第四行进入到位置嵌入，定义了<code>self.pos_embedding = nn.Parameter(torch.randn(1,num_patches + 1, dim))</code>，它初始化了一个和添加了cls_token后张量的后两个维度相同的位置嵌入，按照Transformer论文，应当是正余弦方式进行嵌入，即<span class="math display">\[PE(pos,2i)=\sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]</span> <span class="math display">\[PE(pos,2i+1)=\cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]</span>但这里的初始化，将其设计成了可以训练的参数，与原文不太一样，我们认为网络可以通过参数学习到每个patch之间的相对位置关系。总之，这里将位置嵌入加上来。随后紧接着dropout，这里使用的是emb_dropout，防止过拟合。下面进入到transformer类中，注意，此时的张量形状为[batch_size,patch_num+1, dim]，这是一个贯穿ViT模型始终的重要张量形状。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = <span class="hljs-variable language_">self</span>.transformer(x)<br></code></pre></td></tr></table></figure>这一步，进入transformer类</p><h4 id="transformer类">Transformer类</h4><p>Transfomer类的前向传播： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> attn, ff <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.layers:<br>    x= attn(x)+ x<br>    x= ff(x)+x<br><span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.norm(x)<br></code></pre></td></tr></table></figure>这个结构比较简单，attn就是注意力块，ff是前馈网络，norm在dim大小的维度(最后一维)进行归一化，可以看到self.layers的定义<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(depth):<br>    <span class="hljs-variable language_">self</span>.layers.append(nn.ModuleList([<br>    Attention(dim, heads= heads, dim_head =dim_head,dropout= dropout),<br>    FeedForward(dim, mlp_dim, dropout= dropout)<br>    ]))<br></code></pre></td></tr></table></figure>这里可以看到，att由Attention类构建，ff由FeedForward构建，一共重复depth次，可以理解为将Transformer的encoder部分重复depth次。</p><h4 id="attention类">Attention类</h4><p>这是Transformer最核心最重要的部分。我们一步一步分析Attention块的前向传播：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x= <span class="hljs-variable language_">self</span>.norm(x)<br>qkv=<span class="hljs-variable language_">self</span>.to_qkv(x).chunk( <span class="hljs-number">3</span> , dim =- <span class="hljs-number">1</span> )<br>q, k, v=<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> t: rearrange(t,<span class="hljs-string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h=<span class="hljs-variable language_">self</span>.heads), qkv)<br></code></pre></td></tr></table></figure>首先在最后一个维度进行self.norm(LayerNorm)，然后self.to_qkv(x).chunk(3,dim =-1)这一步，self.to_qkv本质是一个线性层，将张量最后一个维度从dim维映射到dim_head* heads * 3。在最后一个维度进行切分，将1个形如[batch_size, patch_num+1,dim_head * heads * 3]的张量切分为3个[batch_size, patch_num+1, dim_head *heads]张量，形成元组(q, k, v)，分别为query, key,value。通过map将每个[batch_size, patch_num+1, dim_head *heads]张量reshape为形如[batch_size, heads, patch_num+1,dim_head]的张量，形成了真正用于运算的q, k, v向量。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">dots=torch.matmul(q, k.transpose(- <span class="hljs-number">1</span> , - <span class="hljs-number">2</span> )) *<span class="hljs-variable language_">self</span>.scale<br>attn=<span class="hljs-variable language_">self</span>.attend(dots)<br>attn=<span class="hljs-variable language_">self</span>.dropout(attn)<br>out=torch.matmul(attn, v)<br>out=rearrange(out, <span class="hljs-string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)<br></code></pre></td></tr></table></figure>这里使用matmual函数在q,k的最后两个维度进行点积。self.scale参数是dim_head **-0.5。self.attend是在最后一个维度进行softmax，self.scale缩放的目的是为了使softmax的计算结果更加具有区分度，而不是结果集中在1与0附近。dropout后再与v进行点乘，点乘后重排为[batch_size,num_patches+1, heads*num_head]。</p><p>具体而言，这一部分的计算公式为： <span class="math display">\[Attention(q, k, v)=softmax(\frac{qk^T}{\sqrt{dim\_head}})v\]</span>本质是计算了加权后的v，加权的本质是考虑了序列前后之间的关系，<spanclass="math inline">\(qk^T\)</span>正是计算了序列前后的关联性。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.to_out(out)<br></code></pre></td></tr></table></figure> 最后，self.to_out映射到[batch-size, num_patches+1,dim]上。这样，一个attention块之旅就结束了，下面是前馈网络。</p><h4 id="feedforward">FeedForward</h4><p>代码很简单，就只是经过以下一个序列： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.net=nn.Sequential(<br>    nn.LayerNorm(dim),<br>    nn.Linear(dim, hidden_dim),<br>    nn.GELU(),<br>    nn.Dropout(dropout),<br>    nn.Linear(hidden_dim, dim),<br>    nn.Dropout(dropout)<br>    )<br></code></pre></td></tr></table></figure>注意一下经过的是GELU激活函数，据说是在大模型里效果比较好，这里本质上对形状没有调整，这里的hidden_dim就是前面的参数mlp_dim。至此，前馈结束，其运算结果再次进入Attention块不断重复。</p><h4 id="回到vit">回到ViT</h4><p>这里，结束了<code>x = self.transformer(x)</code>，继续ViT类中的forward函数。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">    x= x.mean(dim= <span class="hljs-number">1</span> ) <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.pool== <span class="hljs-string">&#x27;mean&#x27;</span> <span class="hljs-keyword">else</span> x[:, <span class="hljs-number">0</span> ]<br>    x= <span class="hljs-variable language_">self</span>.to_latent(x)<br><span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.mlp_head(x)<br></code></pre></td></tr></table></figure>在第一个维度(num_patches+1)上进行均值(mean情况下)，否则，只取num_patches+1维度的第一个变量(tokens)，即只使用cls_token作为分类结果。不论何种方式，其均形成[batch-size,dim]的张量。to_latent什么都不干，mlp_head将dim映射到num_classes上，展示最终分类结果。</p>]]></content>
    
    
    <categories>
      
      <category>Computer Vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
